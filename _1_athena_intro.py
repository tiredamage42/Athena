
'''
NOTE FOR PEOPLE THAT ARE WELL VERSED IN MACHINE LEARNING:
    I'm aware that Convolutional models would be better suited for these demonstrations,
    but for the sake of simplicity I'll be using simple Feed-Forward Neural Networks.
'''

'''
    Neural nets ( NN ) can be described as function approximators.  
    The purpose of a basic NN is to learn the function that will give you the desired output
    for a particular input, such as labelling a picture as either a dog or a cat.

    The learning process happens by taking various input/output pairs and iteratively 
    finding the function that will produce Y from input X. This is called training.

    Before we go into how that iterative process works, we need to examine what exactly makes up
    the architecture of a basic NN:

    - Layers:
        the major components of a neural net are called "layers".  In general every model
        has at least 3 layers: the input layer, output layer, and at least 1 hidden layer.

        _________                       
        | photo |                       
        |       | => [ Input Layer ] => [ Hidden Layer ] => [ Output Layer (i.e the 'cat' or 'dog' label) ]
        | input |                       
        ---------        

        a numerical representation of the the input data (in this case pixel values) gets fed into the input layer
        then is modified by the hidden layers in order to return the result in the output layer
    
    - Hidden Layer:
        the hidden layers are each made up of 2 dimensional arrays (matrices) of float values,
        often called the `weights`. the matrix multiplication of these weights with the data 
        representation from the previous layer is meant to approximate the function the 
        NN is trying to 'learn'

        in essense waht the NN learns is:
            "what blob of numbers can i multiply with my input to consistently get the correct output?"

    BACKPROPAGATION:
    when we first create the NN these hidden layer weights start off as completely random numbers.

    during the training phase, we repeatedly and randomly supply the developing NN 
    with an input output pair: ( X, Y ) and the training process iteratevly figures out how to adjust
    those random numbers in order to create Y from X, effectively approximating f(X) so that it equals Y.
    
    this happens by taking it's own output prediction ( Z ) and calcualting the difference between 
    that and Y.

    for each number in the weights matrix, it calculates the derivative of this difference with respect to
    each matrix value, and slightly adjusts the number in the direction of the gradient that would
    "minimize" the difference between Z and Y

    This is a very gradual process for each iteration, as the NN needs to be able to generalize,
    so it can deal with inputs it hasn't trained on, showing us that it actually learned
    f(X), and that it didn't just "memorize" the training data.

    luckily we don't have to implement all that calculus and matrix multiplication ourselves!
    we can jsut use third party libraries such as Tensorflow.

    this whole process is referred to as "supervised learning"

    Below we'll run through an explanation of the code in feed_forward_nn.py and 
    
    train a simple feed forward neural net to label iamges of handwritten digits [0 - 9]
    using Tensorflow:

    to run it yourself:
    python feed_forward_nn.py
'''


'''
EXPLAIN DEMO IN feed_forward_nn.py
'''


'''
GANS:
A more advanced type of NN used for different purposes is called a General Adverserial Network (GAN).

GANs are generally used to create data from 'scratch', you've probably seen this in photos of
AI generated faces.

The implementation of these models closely follows the basic Neural Net defined above (an iterative
process of gradually changing the hidden layer weights in order to achieve the optimal result).

the major difference is in the architecture and what the Input/Output pairs are when training.

GANs are made up of 2 different neural nets connected together, a Generator and a Discriminator .
they both ahve seperate "tasks" to learn:

the Generator takes in a random set of numbers as an input and eventually outputs
a 'created' version of a single datapoint (i.e. an image)

the Discriminator takes that generated output as its input, and outputs a binary output of either:
0 if it determines that the input was fake (generated by the Generator) or
1 if it determines that the input was a real data point (i.e. a real picture of someones face)


    [ Random Numbers ] => [ Generator Hidden Layers ] => [ Generator Output ] =>V
                                                                                V
    V<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
    V
    V => [ Discriminator Input ] => [ Discriminator Hidden Layers ] => [ Discriminator Output ('fake' or 'real')]


through the training process, the Discriminator is trained to tell the difference between 
the generated data points and the real data points (using the same process of 
backpropagation detailed above).

while doing that, the Generator is also trained on how to generate images such that the Discriminator
will not be able to tell the difference. It does this through backpropagation as well, except that we
set the target output of the Discriminator to be an output of "real data point" (1) when given an input
generated by the Generator.

as the Discriminator gets better at telling the fake and real data apart, the Generator uses the 
hidden layers of the Discriminaotr to get better at generating more realistic data.

Below we'll go over the code in gan_nn.py that creates and trains a GAN to generate novel images of 
handwritten digits, by having it train on the mnist dataset:

to run it yourself:
python gan_nn.py

'''

'''
EXPLAIN DEMO IN gan_nn.py
'''


'''
ATHENA

Like Prometheus creating humans from clay in Greek mythology, when we create these models,
we are essentailly creating the blueprints (or husks) of the algorithms, 
and letting them grow and change on their own.

In that creation story though, life was breathed into humankind by the goddess Athena.
could we draw inspiration from this part of the myth and apply it to our models?

Machine Learning has been shown to accomplish many amazing feats (sometime even better than humans).
so could we train a model to approximate the method of creating (or breathing life into) another 
neural net model?

GANs have been used to generate images that are strikingly similar to images found in a particular dataset.
Grayscale images are nothing more than 2 dimensional matrices of pixel float values,
much the same as a hidden layer weights on a typical Neural Network!

So, let's see if we can't train a GAN (Athena) to generate 2d matrices to be used as 
weights in another NN model.

theoretically, given enough examples of weights from already trained and working models, 
Athena should be able to create a completely novel set of values in one iteration that would be 
comparable to the values that come from a model trained through the exhaustive backpropogation process

NOTE:
I've never tried this before, and am by no means a mathematician or data scientists, 
so this might be a complete wash, but let's try it and find out!


IMPLEMENTATION:

Goal:
the goal will be for Athena to create the weights of a model that can label the standard
benchmark for image recognition: the MNIST dataset ( a series of images of handwritten numbers from 
0 - 9 ).

first we'll see what the average accuracy is for several (let's say 100) untrained models 
(completely random weight values) on the data set.

If Athena can generate somehting with a noteable increase in accuracy, then we can consider this
experiment a success and think further about optimizations, improvements, or other use cases


Generating Data:
to generate the dataset that Athena will sample from to train, we need to first train
many neural nets to label the MNIST dataset.

all these models will ahve one hidden layer each of the same dimensions, but since they are all
initialized with random values, the trained versions should all have different weight values
from eachother when fully trained as well

once each model is trained to test at 90% accuracy (at least), we'll store the weights
and start over.  when enough weights are stored, the entire dataset is saved to a numpy file


Training:
we will train Athena's Discriminator to tell the difference between a random set of values, and values
that would be an ideal candidate for hidden layer weights that can label the MNIST dataset.

Athena's Generator will hopefully also learn how to generate values that can do so as well!

Testing and debugging throughout training:
On the default graph, we'll have a special testing version of a FeedForward NN that can have its
hidden layer weights assigned to ( TestNN ).

We'll have Athena generate 10 different weights matrices, assign each of them to the TestNN weights,
and test TestNN agaisnt the MNIST dataset. we'll tehn print and track the average accuracy of all 10
tests

'''

'''
EXPLAIN athena_nn.py code...
'''


'''
CONCERNS:
- are the trained weight values of different models necessarily that much differnt
    if they're all trained on the same task?



THINGS TO TRY IN THE FUTURE:
- add support for biases along with weights
- try and generate multi-layer networks with recurrent Athena Model
- try a Cross-Domain Athena GAN to simulate the training process 
    (maybe it can be taught to enhance the accuracy of an alread trained network)
'''